---
title: "Models, statistical inference, and learning"
author: |
  | MACS 33001
  | University of Chicago
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)

library(tidyverse)
library(patchwork)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal(base_size = 18))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}}$$

## Statistical inference

* Process of using data to infer the probability distribution/random variable that generated the data
* Given a sample $X_1, \ldots, X_n \sim F$, how do we infer $F$?
* All parameters? Some parameters? One parameter?

## Parametric vs. nonparametric models

* Statistical model $\xi$ 
* Parametric model

    $$\xi \equiv f(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[ -\frac{1}{2\sigma^2} (x - \mu)^2 \right], \quad \mu \in \Re, \sigma > 0$$
* General form

    $$\xi \equiv f(x; \theta) : \theta \in \Theta$$
* Nonparametric model
    * Infinite set $\xi$

## Parametric estimation

* One dimension
* Two dimensions

## Nonparametric density estimation

* Let $X_1, \ldots, X_n$ be independent observations from a cumulative distribution function (CDF) $F$
* Let $f = F'$ be the probability density function (PDF)
* How to estimate $f$ where $F \in \xi_{\text{ALL}}$ where $\xi_{\text{ALL}} = \{\text{all CDF's} \}$?
* Assume some smoothness on $f$
    * $f \in \xi = \xi_{\text{DENS}} \cap \xi_{\text{SOB}}$
    * $\xi_{\text{DENS}}$ is the set of all PDFs and

        $$\xi_{\text{SOB}} \equiv f: \int (f''(x))^2 dx < \infty$$

## Nonparametric density estimation

* General form

    $$g(x) = \frac{1}{nh} \sum_{i = 1}^n f(x)$$

## Infant mortality

```{r infant-data}
infant <- read_csv("../data/infant.csv") %>%
  # remove non-countries
  filter(is.na(`Value Footnotes`) | `Value Footnotes` != 1) %>%
  select(`Country or Area`, Year, Value) %>%
  rename(country = `Country or Area`,
         year = Year,
         mortal = Value)
```

```{r infant-hist}
ggplot(infant, aes(mortal)) +
  geom_histogram(bins = 30, boundary = 0) +
  labs(title = "Histogram of infant mortality rate for 195 nations",
       subtitle = "30 bins, origin = 0",
       x = "Infant mortality rate (per 1,000)",
       y = "Frequency")
```

## Gaussian kernel

$$f(x) = \frac{1}{\sqrt{2 \pi}}\exp\left[-\frac{1}{2} x^2 \right]$$

```{r gaussian, fig.asp = .8}
x <- rnorm(1000)

{
  qplot(x, geom = "blank") +
    stat_function(fun = dnorm) +
    labs(title = "Gaussian (normal) kernel",
         x = NULL,
         y = NULL)
} +
{
  ggplot(infant, aes(mortal)) +
    geom_density(kernel = "gaussian") +
    labs(title = "Infant mortality rate for 195 nations",
         x = "Infant mortality rate (per 1,000)",
         y = "Density")
}
```

## Rectangular (uniform) kernel

$$f(x) = \frac{1}{2} \mathbf{1}_{\{ |x| \leq 1 \} }$$

```{r uniform, fig.asp = .8}
x <- runif(1000, -1.5, 1.5)
x_lines <- tribble(
  ~x, ~y, ~xend, ~yend,
  -1, 0, -1, .5,
  1, 0, 1, .5
)

{
  qplot(x, geom = "blank") +
    stat_function(fun = dunif, args = list(min = -1), geom = "step") +
    # geom_segment(data = x_lines, aes(x = x, y = y, xend = xend, yend = yend)) +
    labs(title = "Rectangular kernel",
         x = NULL,
         y = NULL)
} +
{
  ggplot(infant, aes(mortal)) +
    geom_density(kernel = "rectangular") +
    labs(title = "Infant mortality rate for 195 nations",
         x = "Infant mortality rate (per 1,000)",
         y = "Density")
}
```

## Comparison of kernels {.scrollable}

```{r kernels}
# define custom kernel functions
triangular <- function(x) {
  (1 - abs(x)) * ifelse(abs(x) <= 1, 1, 0)
}

biweight <- function(x) {
  (15 / 16) * (1 - x^2)^2 * ifelse(abs(x) <= 1, 1, 0)
}

epanechnikov <- function(x) {
  (15 / 16) * (1 - x^2)^2 * ifelse(abs(x) <= 1, 1, 0)
}

qplot(x, geom = "blank") +
  stat_function(aes(color = "Gaussian"), fun = dnorm) +
  stat_function(aes(color = "Epanechnikov"), fun = epanechnikov) +
  stat_function(aes(color = "Rectangular"), fun = dunif,
                args = list(min = -1), geom = "step") +
  stat_function(aes(color = "Triangular"), fun = triangular) +
  stat_function(aes(color = "Biweight"), fun = biweight) +
  scale_color_brewer(type = "qual") +
  labs(x = NULL,
       y = NULL,
       color = NULL) +
  theme(legend.position = c(0.04, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(fill = "white"))

ggplot(infant, aes(mortal)) +
  geom_density(aes(color = "Gaussian"), kernel = "gaussian") +
  geom_density(aes(color = "Epanechnikov"), kernel = "epanechnikov") +
  geom_density(aes(color = "Rectangular"), kernel = "rectangular") +
  geom_density(aes(color = "Triangular"), kernel = "triangular") +
  geom_density(aes(color = "Biweight"), kernel = "biweight") +
  scale_color_brewer(type = "qual") +
  labs(title = "Density estimators of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density",
       color = "Kernel") +
  theme(legend.position = c(0.96, 1),
        legend.justification = c(1, 1),
        legend.background = element_rect(fill = "white"))
```

## Regression

* Suppose we observe pairs of data $(X_1, Y_1), \ldots, (X_n, Y_n)$
* $X$
* $Y$
* $r(x) = \E(Y | X = x)$
* Parametric regression
* Nonparametric regression

## Naive non-parametric regression

```{r np-data}
n <- 1000000
wage <- data_frame(educ = rpois(n, lambda = 12),
                   age = rpois(n, lambda = 40),
                   prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

ggplot(wage, aes(wage)) +
  geom_histogram(binwidth = 5) +
  labs(title = "Histogram of simulated income data",
       subtitle = "Binwidth = 5",
       x = "Income, in thousands of dollars",
       y = "Frequency count")
```

## Naive non-parametric regression {.scrollable}

$$\mu = E(\text{Income}|\text{Education}) = f(\text{Education})$$

```{r np-wage-cond}
wage %>%
  group_by(educ) %>%
  summarize(mean = mean(wage),
            sd = sd(wage)) %>%
  ggplot(aes(educ, mean, ymin = mean - sd, ymax = mean + sd)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "Conditional income, by education level",
       subtitle = "Plus/minus SD",
       x = "Education level",
       y = "Income, in thousands of dollars")

wage %>%
  filter(educ == 12) %>%
  ggplot(aes(wage)) +
  geom_density() +
  geom_vline(xintercept = mean(wage$wage[wage$educ == 12]), linetype = 2) +
  labs(title = "Conditional distribution of income for education = 12",
       subtitle = str_c("Mean income = ", formatC(mean(wage$wage[wage$educ == 12]), digits = 3)),
       x = "Income, in thousands of dollars",
       y = "Frequency count")
```

## Naive non-parametric regression

$$\mu = E(Y|x) = f(x)$$

* Binning

## Naive non-parametric regression

```{r prestige}
# get data
prestige <- read_csv("../data/prestige.csv")
```

```{r prestige-5bins, dependson="prestige"}
# bin into 5 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 6)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
  as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[2:(length(.)-1)] %>%
  as_tibble

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 5",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

## Naive non-parametric regression

```{r prestige-50bins, dependson="prestige"}
# bin into 50 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 51)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
  as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[2:(length(.)-1)] %>%
  as_tibble

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2, alpha = .25) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 50",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

## Naive non-parametric regression

$$X_1 \in \{1, 2, \dots ,10 \}$$
$$X_2 \in \{1, 2, \dots ,10 \}$$
$$X_3 \in \{1, 2, \dots ,10 \}$$

* $10^3 = 1000$ possible combinations of the explanatory variables and $1000$ conditional expectations of $Y$ given $X$:

    $$\mu = E(Y|x_1, x_2, x_3) = f(x_1, x_2, x_3)$$

## Naive non-parametric regression {.scrollable}

```{r wage-sim-describe}
ggplot(wage, aes(educ)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Education",
       y = "Frequency count")

ggplot(wage, aes(age)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Age",
       y = "Frequency count")

ggplot(wage, aes(prestige)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Job prestige",
       y = "Frequency count")
```

## Naive non-parametric regression

```{r wage-sim-np}
wage_np <- wage %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                          wage_sd = NA,
                                          n = 0))

# number of unique combos 
wage_unique <- nrow(wage_np)

# n for each unique combo
ggplot(wage_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

## Naive non-parametric regression

```{r wage-sim-np-ten}
n <- 10000000
wage10 <- data_frame(educ = rpois(n, lambda = 12),
                   age = rpois(n, lambda = 40),
                   prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

wage10_np <- wage10 %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                          wage_sd = NA,
                                          n = 0))

# number of unique combos 
wage10_unique <- nrow(wage10_np)

# n for each unique combo
ggplot(wage10_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

## Point estimates

* Single "best guess" of some quantity of interest
    * Parameter in a parametric model
    * CDF $F$
    * PDF $f$
    * Regression function $r$
* Point estimate of $\theta$ is $\hat{\theta}$ or $\hat{\theta}_n$
    * $\theta$ is a fixed, unknown quantity
    * $\hat{\theta}$ is a random variable
* Let $X_1, \ldots, X_n$ be $n$ IID data points from some distribution $F$. A point estimator $\hat{\theta}_n$ of a paramater $\theta$ is some function of $X_1, \ldots, X_n$:

    $$\hat{\theta}_n = g(X_1, \ldots, X_n)$$

## Properties of point estimates

* Bias
    $$\text{bias}(\hat{\theta}_n) = \E_\theta (\hat{\theta_n}) - \theta$$
* Unbiasedness
    * Is this necessary?
* Consistency

## Properties of point estimates

* Sampling distribution
* Standard error

    $$\se = \se(\hat{\theta}_n) = \sqrt{\Var (\hat{\theta}_n)}$$
    * Depends on the unknown $F$
    * Usually estimated from the data ($\widehat{\se}$)
* Mean squared error

    $$
    \begin{align}
    \text{MSE} &= \E_\theta (\hat{\theta}_n - \theta)^2 \\
    &= \text{bias}^2(\hat{\theta}_n) + \Var_\theta (\hat{\theta}_n)
    \end{align}
    $$

## Properties of point estimates

* Estimators are approximately Normally distributed

    $$\frac{\hat{\theta}_n - \theta}{\se} \leadsto N(0,1)$$

## Example: Bernoulli distributed random variable {.scrollable}

* Let $X_1, \ldots, X_n ~ \text{Bernoulli}(\pi)$
* Let $\hat{\pi}_n = \frac{1}{n} \sum_{i=1}^n X_i$. Then

    $$\E(\hat{\pi}_n) = \frac{1}{n} \sum_{i=1}^n \E(X_i) = \pi$$

    so $\hat{\pi}_n$ is unbiased
* Standard error is

    $$\se = \sqrt{\Var (\hat{\pi}_n)} = \sqrt{\frac{\pi (1 - \pi)}{n}}$$

    $$\widehat{\se} = \sqrt{\frac{\hat{\pi} (1 - \hat{\pi})}{n}}$$

* $\E_\pi (\hat{\pi}_n) = \pi$ so $\text{bias} = \pi - \pi = 0$

    $$
    \begin{align}
    \text{bias}(\hat{\pi}_n) &= \E_\pi (\hat{\pi}) - \pi \\
    &= \pi - \pi \\
    &= 0
    \end{align}
    $$
* Consistency
    $$\se = \sqrt{\frac{\pi (1 - \pi)}{n}} \rightarrow 0$$

## Confidence sets

* $1 - \alpha$ confidence interval for $\theta$ is an interval $C_n = (a,b)$
    * $a = a(X_1, \ldots, X_n)$
    * $b = b(X_1, \ldots, X_n)$

        $$\Pr_{\theta} (\theta \in C_n) \geq 1 - \alpha, \quad \forall \theta \in \Theta$$
* $(a,b)$ traps $\theta$ with probability $1- \alpha$

## Caution interpreting confidence intervals

* $C_n$ is random and $\theta$ is fixed
* 95\% confidence intervals corresponding to $\alpha = 0.05$
* A confidence interval is not a probability statement about $\theta$

## Proper interpretation

> On day 1, you collect data and construct a 95\% confidence interval for a parameter $\theta_1$. On day 2, you collect new data and construct a 95\% confidence interval for a parameter $\theta_2$. You continue this way constructing confidence intervals for a sequence of unrelated parameters $\theta_1, \theta_2, \ldots$. Then 95\% of your intervals will trap the true parameter value.

## Constructing confidence intervals {.scrollable}

* Approximately Normal - use the Normal distribution
* Suppose that $\hat{\theta}_n \approx N(\theta, \widehat{\se}^2)$
* Let $\Phi$ be the CDF of a standard Normal distribution

    $$z_{\frac{\alpha}{2}} = \Phi^{-1} \left(1 - \frac{\alpha}{2} \right)$$

    $$\Pr (Z > \frac{\alpha}{2}) = \frac{\alpha}{2}$$

    $$\Pr (-z_{\frac{\alpha}{2}} \leq Z \leq z_{\frac{\alpha}{2}}) = 1 - \alpha$$

    where $Z \sim N(0,1)$
* Let

    $$C_n = (\hat{\theta}_n - z_{\frac{\alpha}{2}} \widehat{\se}, \hat{\theta}_n + z_{\frac{\alpha}{2}} \widehat{\se})$$
* Then

    $$
    \begin{align}
    \Pr_\theta (\theta \in C_n) &= \Pr_\theta (\hat{\theta}_n - z_{\frac{\alpha}{2}} \widehat{\se} < \theta < \hat{\theta}_n + z_{\frac{\alpha}{2}} \widehat{\se}) \\
    &= \Pr_\theta (- z_{\frac{\alpha}{2}} < \frac{\hat{\theta}_n - \theta}{\widehat{\se}} < z_{\frac{\alpha}{2}}) \\
    &\rightarrow \Pr ( - z_{\frac{\alpha}{2}} < Z < z_{\frac{\alpha}{2}}) \\
    &= 1 - \alpha
    \end{align}
    $$

* $\alpha = 0.05$ and $z_{\frac{\alpha}{2}} = 1.96 \approx 2$

## Actual vs. approximate confidence intervals {.scrollable}

* Let $X_1, \ldots, X_n \sim \text{Bernoulli}(\pi)$ and let $\hat{\pi}_n = \frac{1}{n} \sum_{i=1}^n X_i$
* Let $C_n = (\hat{\pi}_n - \epsilon_n, \hat{\pi}_n + \epsilon_n)$ where $\epsilon_n^2 = \frac{\log(\frac{2}{\alpha})}{2n}$
* From this,

    $$\Pr (\pi \in C_n \geq 1 - \alpha)$$
* $C_n$ is a precise $1 - \alpha$ confidence interval
* Approximate confidence interval

    $$
    \begin{align}
    \Var (\hat{\pi}_n) &= \frac{1}{n^2} \sum_{i=1}^n \Var(X_i) \\
    &= \frac{1}{n^2} \sum_{i=1}^n \pi(1 - \pi) \\
    &= \frac{1}{n^2} n\pi(1 - \pi) \\
    &= \frac{\pi(1 - \pi)}{n} \\
    \se &= \sqrt{\frac{\pi(1 - \pi)}{n}} \\
    \widehat{\se} &= \sqrt{\frac{\hat{\pi}(1 - \hat{\pi})}{n}}
    \end{align}
    $$

    $$\hat{\pi}_n \pm z_{\frac{\alpha}{2}} \widehat{\se} = \hat{\pi}_n \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\hat{\pi}(1 - \hat{\pi})}{n}}$$

## Hypothesis testing

* Default theory
* Null vs. alternative hypothesis
* Sufficient evidence to reject the null hypothesis?
* Let

    $$X_1, \ldots, X_n \sim \text{Bernoulli}(\pi)$$
* Is it a fair coin?
    * $H_0: \pi = 0.5$
    * $H_1: \pi \neq 0.5$
    * Reasonable to reject $H_0$ if

        $$T = | \hat{\pi}_n - 0.5|$$
        is large
        