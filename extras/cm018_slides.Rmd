---
title: "OLS: Estimator and diagnostics"
author: |
  | MACS 33001
  | University of Chicago
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)

library(tidyverse)
library(patchwork)
library(broom)
library(modelr)
library(stringr)
library(ISLR)
library(titanic)
library(rcfss)
library(haven)
library(car)
library(lmtest)

options(digits = 3)
set.seed(1234)
base_size <- 18
theme_set(theme_minimal(base_size = base_size))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

## Regression

* Regression
* Response variable $Y$
* Covariate $X$
* Regression function

    $$r(x) = \E (Y | X = x) = \int y f(y|x) dy$$

* Estimate the form

    $$(Y_1, X_1), \ldots, (Y_n, X_n) \sim F_{X,Y}$$

## Simple linear regression

$$r(x) = \beta_0 + \beta_1 x$$

* $X_i$ is one-dimensional
* $r(x)$ assumed to be linear
* $\Var (\epsilon_i | X = x) = \sigma^2$ does not depend on $x$

## Linear regression model {.scrollable}

$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

* $\E (\epsilon_i | X_i) = 0$
* $\Var (\epsilon_i | X_i) = \sigma^2$
* Unknown $\beta_0$, $\beta_1$, and $\sigma^2$
* $\hat{\beta}_0$ and $\hat{\beta}_1$
* Fitted line

    $$\hat{r}(x) = \hat{\beta}_0 + \hat{\beta}_1 x$$

* Fitted values

    $$\hat{Y}_i = \hat{r}(X_i)$$

* Residuals

    $$\hat{\epsilon}_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x)$$

* Residual sum of squares

    $$RSS = \sum_{i=1}^n \hat{\epsilon}_i^2$$

## Estimation strategy

```{r sim-plot}
ggplot(sim1, aes(x, y)) + 
  geom_point()
```

## Estimation strategy

```{r sim-random-fit}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()
```

## Estimation strategy

* Unbiased
    * $E(\hat{\beta}) = \beta$
* Efficient
    * $\min(Var(\hat{\beta}))$

## Least squares estimator

* Values $\hat{\beta}_0, \hat{\beta}_1$ that

    $$\min(RSS)$$

$$
\begin{aligned}
\hat \beta_1 & = \dfrac{ \sum_{i=1}^n Y_iX_i  - \bar{Y}_n \sum_{i=1}^nX_i}{ \sum_{i=1}^n X_i^2 - \bar{X}_n\sum_{i=1}^nX_i}\\
 \hat \beta_0 & = \bar{Y}_n - \hat{\beta}_1 \bar{X}_n \\
 \hat{\sigma}^2 &= \left( \frac{1}{n - 2} \right) \sum_{i=1}^n \hat{\epsilon}_i^2
\end{aligned}
$$

## Least squares estimator

```{r sim-lm}
sim1_mod <- lm(y ~ x, data = sim1)

dist2 <- sim1 %>%
  add_predictions(sim1_mod) %>%
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge
  )

ggplot(dist2, aes(x1, y)) + 
  geom_smooth(method = "lm", color = "grey40", se = FALSE) +
  geom_point(color = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), color = "#3366FF")
```

## Multivariate formulation

$$\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u}$$

* $\mathbf{Y}$: $N\times 1$ vector
* $\mathbf{X}$: $N \times K$ matrix
* $\boldsymbol{\beta}$: $K \times 1$ vector
* $\mathbf{u}$: $N\times 1$ vector
* $i \in \{1,\ldots,N \}$
* $k \in \{1,\ldots,K \}$

## Multivariate formulation

$$(\mathbf{X'X})^{-1}\mathbf{X'Y} = \boldsymbol{\beta}$$

## Maximum likelihood estimator {.scrollable}

* $\epsilon_i | X_i \sim N(0, \sigma^2)$

    $$Y_i | X_i \sim N(\mu_i, \sigma^2)$$

    * $\mu_i = \beta_0 + \beta_1 X_i$
* Likelihood function

    $$
    \begin{align}
    \prod_{i=1}^n f(X_i, Y_i) &= \prod_{i=1}^n f_X(X_i) f_{Y | X} (Y_i | X_i) \\
    &= \prod_{i=1}^n f_X(X_i) \times \prod_{i=1}^n f_{Y | X} (Y_i | X_i) \\
    &= \Lagr_1 \times \Lagr_2
    \end{align}
    $$
    
    $$
    \begin{align}
    \Lagr_1 &= \prod_{i=1}^n f_X(X_i) \\
    \Lagr_2 = \prod_{i=1}^n f_{Y | X} (Y_i | X_i)
    \end{align}
    $$

* Conditional likelihood
    
    $$
    \begin{align}
    \Lagr_2 &\equiv \Lagr(\beta_0, \beta_1, \sigma^2) \\
    &= \prod_{i=1}^n f_{Y | X}(Y_i | X_i) \\
    &\propto \frac{1}{\sigma} \exp \left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n (Y_i - \mu_i)^2 \right\}
    \end{align}
    $$

* Conditional log-likelihood

    $$\lagr(\beta_0, \beta_1, \sigma^2) = -n \log(\sigma) - \frac{1}{2\sigma^2} \left( Y_i - (\beta_0 + \beta_1 X_i) \right)^2$$
    
    * $\max \left( \lagr(\beta_0, \beta_1, \sigma^2) \right)$
    * Equivalent to $\min(RSS)$

        $$RSS = \sum_{i=1}^n \hat{\epsilon}_i^2 = \sum_{i=1}^n \left( Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x) \right)$$

## Properties of the least squares estimator

* Conditional on $X^n = (X_1, \ldots, X_n)$
* $\hat{\beta}^T = (\hat{\beta}_0, \hat{\beta}_1)^T$

    $$
    \begin{align}
    \E (\hat{\beta} | X^n) &= \begin{pmatrix}
      \beta_0 \\
      \beta_1
     \end{pmatrix} \\
     \Var (\hat{\beta} | X^n) &= \frac{\sigma^2}{n s_X^2}  \begin{pmatrix}
      \frac{1}{n} \sum_{i=1}^n X_i^2 & -\bar{X}^n \\
      -\bar{X}^n & 1
     \end{pmatrix}
    \end{align}
    $$

    $$s_X^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}_n)^2$$

## Properties of the least squares estimator

$$
\begin{align}
\widehat{\se} (\hat{\beta}_0) &= \frac{\hat{\sigma}}{s_X \sqrt{n}} \sqrt{\frac{ \sum_{i=1}^n X_i^2}{n}} \\
\widehat{\se} (\hat{\beta}_1) &= \frac{\hat{\sigma}}{s_X \sqrt{n}}
\end{align}
$$

## Properties of the least squares estimator {.scrollable}

1. Consistency

    $$\hat{\beta}_0 \xrightarrow{P} \beta_0 \, \text{and} \, \hat{\beta}_1 \xrightarrow{P} \beta_1$$
    
1. Asymptotic normality

    $$\frac{\hat{\beta}_0 - \beta_0}{\widehat{\se}(\hat{\beta}_0)} \leadsto N(0,1) \, \text{and} \, \frac{\hat{\beta}_1 - \beta_1}{\widehat{\se}(\hat{\beta}_1)} \leadsto N(0,1)$$

1. Approximate $1 - \alpha$ confidence intervals for $\beta_0$ and $\beta_1$ are

    $$\hat{\beta}_0 \pm z_{\alpha / 2} \widehat{\se}(\hat{\beta}_0) \, \text{and} \, \hat{\beta}_1 \pm z_{\alpha / 2} \widehat{\se}(\hat{\beta}_1)$$
    
1. The Wald test for testing $H_0: \beta_1 = 0$ versus $H_1: \beta_1 \neq 0$ is reject $H_0$ if $\mid W \mid > z_{\alpha / 2}$ where

    $$W = \frac{\hat{\beta}_1}{\widehat{\se}(\hat{\beta}_1)}$$

## Assumptions of linear regression models

* Linearity
* Constant variance
* Normality
* Independence
* Fixed $X$
* $X$ is not invariant

## Linearity

$$\E(\epsilon_i) \equiv E(\epsilon_i | X_i) = 0$$

$$
\begin{aligned}
\mu_i \equiv E(Y_i) \equiv E(Y | X_i) &= E(\beta_0 + \beta_1 X_i + \epsilon_i) \\
\mu_i &= \beta_0 + \beta_1 X_i + E(\epsilon_i) \\
\mu_i &= \beta_0 + \beta_1 X_i + 0 \\
\mu_i &= \beta_0 + \beta_1 X_i
\end{aligned}
$$

## Constant variance

* The variance of the errors is the same regardless of the values of $X$

    $$\Var(\epsilon_i | X_i) = \sigma^2$$

## Normality

* The errors are assumed to be normally distributed

    $$\epsilon_i \mid X_i \sim N(0, \sigma^2)$$

## Independence

* Observations are sampled independently from one another
* Any pair of errors $\epsilon_i$ and $\epsilon_j$ are independent for $i \neq j$.

## Fixed $X$

* $X$ is assumed to be fixed or measured without error and independent of the error
* Experimental design
* Observational study

    $$\epsilon_i \sim N(0, \sigma^2), \text{for } i = 1, \dots, n$$

## $X$ is not invariant

```{r invariant}
data_frame(x = 1,
           y = rnorm(10)) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "You cannot regress this",
       subtitle = "Slope is undefined")
```

## Handling violations of assumptions

* Ooopsie
* Inference can be
    * Tricky
    * Biased
    * Inefficient
    * Error prone
* Move to more robust inferential method
* Diagnose assumption violations and solve them in the OLS framework

## Unusual and influential data

* Outliers
    * Unusual observations
    * Due to $Y_i$
    * Due to $X_i$
    * Some combination of the two
* Potential disproportionate influence on the regression model

## Terms

* Outlier
* Leverage
* Discrepancy
* Influence

    $$\text{Influence} = \text{Leverage} \times \text{Discrepancy}$$

## Unusual and influential data

```{r flintstones-sim}
flintstones <- tribble(
  ~name,    ~x, ~y,
  "Barney", 13, 75,
  "Dino",   24, 300,
  "Betty",  14, 250,
  "Fred",   10, 220,
  "Wilma",  8,  210
)

ggplot(flintstones, aes(x, y, label = name)) +
  geom_smooth(data = filter(flintstones, name %in% c("Wilma", "Fred", "Betty")),
              method = "lm", se = FALSE, fullrange = TRUE,
              aes(linetype = "Betty + Fred + Wilma",
                  color = "Betty + Fred + Wilma")) +
  geom_smooth(data = filter(flintstones, name != "Dino"),
              method = "lm", se = FALSE, fullrange = TRUE,
              aes(linetype = "Barney + Betty + Fred + Wilma",
                  color = "Barney + Betty + Fred + Wilma")) +
  geom_smooth(data = filter(flintstones, name != "Barney"),
              method = "lm", se = FALSE, fullrange = TRUE,
              aes(linetype = "Betty + Dino + Fred + Wilma",
                  color = "Betty + Dino + Fred + Wilma")) +
  scale_linetype_manual(values = c(3,2,1)) +
  scale_color_brewer(type = "qual") +
  geom_point(size = 2) +
  ggrepel::geom_label_repel() +
  labs(linetype = NULL,
       color = NULL) +
  theme(legend.position = "bottom") + 
  guides(color = guide_legend(nrow = 3, reverse = TRUE),
         linetype = guide_legend(nrow = 3, reverse = TRUE))
```

## Measuring leverage

* Hat statistic

    $$h_i = \frac{1}{n} + \frac{(X_i - \bar{X})^2}{\sum_{j=1}^{n} (X_{j} - \bar{X})^2}$$

* Measures the contribution of observation $Y_i$ to the fitted value $\hat{Y}_j$
* Solely a function of $X$
* Larger values indicate higher leverage

## Measuring discrepancy

* Use the residuals?
* Standardized residual

    $$\hat{\epsilon}_i ' \equiv \frac{\hat{\epsilon}_i}{S_{E} \sqrt{1 - h_i}}$$

    * $S_E = \sqrt{\frac{\hat{\epsilon}_i^2}{(n - k - 1)}}$
* Studentized residual

    $$\hat{\epsilon}_i^{\ast} \equiv \frac{\hat{\epsilon}_i}{S_{E(-i)} \sqrt{1 - h_i}}$$

## Measuring influence

* $\text{DFBETA}_{ij}$

    $$D_{ij} = \hat{\beta_1j} - \hat{\beta}_{1j(-i)}, \text{for } i=1, \dots, n \text{ and } j = 0, \dots, k$$

* $\text{DFBETAS}_{ij}$

    $$D^{\ast}_{ij} = \frac{D_{ij}}{SE_{-i}(\beta_{1j})}$$

* Cook's D

    $$D_i = \frac{\hat{\epsilon}^{'2}_i}{k + 1} \times \frac{h_i}{1 - h_i}$$

## Visualizing leverage, discrepancy, and influence

* Number of federal laws struck down by the U.S. Supreme Court in each Congress

1. Age
1. Tenure
1. Unified

## OLS model

```{r dahl}
# read in data and estimate model
dahl <- read_dta("../data/LittleDahl.dta")
dahl_mod <- lm(nulls ~ age + tenure + unified, data = dahl)
tidy(dahl_mod)
```

## Outliers? {.scrollable}

```{r dahl-time}
dahl <- dahl %>%
  mutate(year = congress * 2 + 1787)

ggplot(dahl, aes(year, nulls)) +
  geom_line() +
  geom_vline(xintercept = 1935, linetype = 2) +
  labs(x = "Year",
       y = "Congressional laws struck down")

ggplot(dahl, aes(year, age)) +
  geom_line() +
  geom_vline(xintercept = 1935, linetype = 2) +
  labs(x = "Year",
       y = "Mean age of justices on the Court")
```

## Outliers?

```{r bubble}
# add key statistics
dahl_augment <- dahl %>%
  mutate(hat = hatvalues(dahl_mod),
         student = rstudent(dahl_mod),
         cooksd = cooks.distance(dahl_mod))

# draw bubble plot
ggplot(dahl_augment, aes(hat, student)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_point(aes(size = cooksd), shape = 1) +
  geom_text(data = dahl_augment %>%
              arrange(-cooksd) %>%
              slice(1:10),
            aes(label = Congress)) +
  scale_size_continuous(range = c(1, 20)) +
  labs(x = "Leverage",
       y = "Studentized residual") +
  theme(legend.position = "none")
```

## Hat-values

* Anything exceeding twice the average $\bar{h} = \frac{k + 1}{n}$ is noteworthy
    
    ```{r hat-sig}
    dahl_augment %>%
      filter(hat > 2 * mean(hat))
    ```

## Studentized residuals

* Anything outside of the range $[-2,2]$ is discrepant
    
    ```{r resid-sig}
    dahl_augment %>%
      filter(abs(student) > 2)
    ```

## Influence

* $D_i > \frac{4}{n - k - 1}$
    
    ```{r cooksd-sig}
    dahl_augment %>%
      filter(cooksd > 4 / (nrow(.) - (length(coef(dahl_mod)) - 1) - 1))
    ```

## How to treat unusual observations

* Mistake
* Weird observation
* Why is it weird?

## Omit unusual observations {.scrollable}

```{r dahl-reestimate}
dahl_omit <- dahl %>%
  filter(!(congress %in% c(74, 98, 104)))

dahl_omit_mod <- lm(nulls ~ age + tenure + unified, data = dahl_omit)

coefplot::multiplot(dahl_mod, dahl_omit_mod,
                    names = c("All observations",
                              "Omit outliers")) +
  theme(legend.position = "bottom")

# rsquared values
rsquare(dahl_mod, dahl)
rsquare(dahl_omit_mod, dahl_omit)

# rmse values
rmse(dahl_mod, dahl)
rmse(dahl_omit_mod, dahl_omit)
```

## Non-normally distributed errors

$$\epsilon_i | X_i \sim N(0, \sigma^2)$$

* Under CLT, inference based on the least-squares estimator is approximately valid under broad conditions
* Violation does not effect validity
* Violation effects efficiency

## Detecting non-normally distributed errors {.scrollable}

```{r slid}
(slid <- read_tsv("http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/SLID-Ontario.txt"))

slid_mod <- lm(compositeHourlyWages ~ sex + yearsEducation + age, data = slid)
tidy(slid_mod)

car::qqPlot(slid_mod)
```

## Detecting non-normally distributed errors

```{r slid-density}
augment(slid_mod, slid) %>%
  mutate(.student = rstudent(slid_mod)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust = .5) +
  labs(x = "Studentized residuals",
       y = "Estimated density")
```

## Fixing non-normally distributed errors

* Power transformations
* Log transformations

## Fixing non-normally distributed errors {.scrollable}

```{r slid-log}
slid <- slid %>%
  mutate(wage_log = log(compositeHourlyWages))

slid_log_mod <- lm(wage_log ~ sex + yearsEducation + age, data = slid)
tidy(slid_log_mod)

car::qqPlot(slid_log_mod)

augment(slid_log_mod, slid) %>%
  mutate(.student = rstudent(slid_log_mod)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust = .5) +
  labs(x = "Studentized residuals",
       y = "Estimated density")
```

## Non-constant error variance

* Homoscedasticity

    $$\text{Var}(\epsilon_i) = \sigma^2$$
    
    $$\widehat{\se}(\hat{\beta}_{1j}) = \sqrt{\hat{\sigma}^{2} (X'X)^{-1}_{jj}}$$

* Heteroscedasticity

## Detecting heteroscedasticity

$$Y_i = 2 + 3X_i + \epsilon$$

$$\epsilon_i \sim N(0,1)$$

```{r sim-homo, fig.asp = .7}
sim_homo <- data_frame(x = runif(1000, 0, 10),
                       y = 2 + 3 * x + rnorm(1000, 0, 1))
sim_homo_mod <- glm(y ~ x, data = sim_homo)

sim_homo %>%
  add_predictions(sim_homo_mod) %>%
  add_residuals(sim_homo_mod) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Homoscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")
```

## Detecting heteroscedasticity

$$Y_i = 2 + 3X_i + \epsilon_i$$

$$\epsilon_i \sim N(0,\frac{X}{2})$$

```{r sim-hetero, fig.asp = .7}
sim_hetero <- data_frame(x = runif(1000, 0, 10),
                       y = 2 + 3 * x + rnorm(1000, 0, (x / 2)))
sim_hetero_mod <- glm(y ~ x, data = sim_hetero)

sim_hetero %>%
  add_predictions(sim_hetero_mod) %>%
  add_residuals(sim_hetero_mod) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Heteroscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")
```

## Breusch-Pagan test

* Estimate an OLS model and obtain the squared residuals $\hat{\epsilon}^2$
* Regress $\hat{\epsilon}^2$ against:
    * All the $k$ variables you think might be causing the heteroscedasticity
* Calculate $R^2_{\hat{\epsilon}^2}$ for the residual model and multiply it by the number of observations $n$
    * $\chi^2_{(k-1)}$ distribution

## Breusch-Pagan test

```{r breusch-pagan}
bptest(sim_homo_mod)
bptest(sim_hetero_mod)
```

## Weighted least squares regression {.scrollable}

* $\epsilon_i \sim N(0, \sigma_i^2)$

    $$
    \begin{bmatrix}
        \sigma_1^2       & 0 & 0 & 0 \\
        0       & \sigma_2^2 & 0 & 0 \\
        0       & 0 & \ddots & 0 \\
        0       & 0 & 0 & \sigma_n^2 \\
    \end{bmatrix}
    $$

* $w_i = \frac{1}{\sigma_i^2}$
    
    $$
    \mathbf{W} =
    \begin{bmatrix}
        \frac{1}{\sigma_1^2}       & 0 & 0 & 0 \\
        0       & \frac{1}{\sigma_2^2} & 0 & 0 \\
        0       & 0 & \ddots & 0 \\
        0       & 0 & 0 & \frac{1}{\sigma_n^2} \\
    \end{bmatrix}
    $$

* Traditional regression estimator

    $$\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{y}$$

* Substitute with weighting matrix

    $$\hat{\boldsymbol{\beta}} = (\mathbf{X}' \mathbf{W} \mathbf{X})^{-1} \mathbf{X}' \mathbf{W} \mathbf{y}$$

    $$\sigma_{i}^2 = \frac{\sum(w_i \hat{\epsilon}_i^2)}{n}$$

## Estimating the weights $W_i$

1. Use the residuals from a preliminary OLS regression to obtain estimates of the error variance within different subsets of observations
1. Model the weights as a function of observable variables in the model

## Weighted least squares regression

```{r wls}
# do we have heteroscedasticity?
slid_mod <- glm(compositeHourlyWages ~ sex + yearsEducation + age, data = slid)

slid %>%
  add_predictions(slid_mod) %>%
  add_residuals(slid_mod) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Heteroscedastic variance of error terms",
       subtitle = "Hourly wages ~ Sex + Education + Age",
       x = "Predicted values",
       y = "Residuals")
```

## Weighted least squares regression

```{r wls-2}
# convert residuals to weights
weights <- 1 / residuals(slid_mod)^2

slid_wls <- lm(compositeHourlyWages ~ sex + yearsEducation + age, data = slid, weights = weights)

tidy(slid_mod)
tidy(slid_wls)
```

## Corrections for the variance-covariance estimates

* Correct standard errors only
* Huber-White (robust) standard errors

## Corrections for the variance-covariance estimates

```{r huber-white}
hw_std_err <- hccm(slid_mod, type = "hc1") %>%
  diag %>%
  sqrt

tidy(slid_mod) %>%
  mutate(std.error.rob = hw_std_err)
```

## Non-linearity in the data

* Assume $\E (\epsilon_i) = 0$
* Implies the regression line accurately reflects the relationship between $X$ and $Y$
* Nonlinearity
    * Nonlinear relationship
    * Conditional relationship

## Partial residual

$$\hat{\epsilon}_i^{(j)} = \hat{\epsilon}_i + \hat{\beta}_j X_{ij}$$

## Partial residual plots {.scrollable}

```{r part-resid-plot}
# get partial resids
slid_resid <- residuals(slid_log_mod, type = "partial") %>%
  as_tibble
names(slid_resid) <- str_c(names(slid_resid), "_resid")

slid_diag <- augment(slid_log_mod, slid) %>%
  bind_cols(slid_resid)

ggplot(slid_diag, aes(age, age_resid)) +
  geom_point(alpha = .1) +
  geom_smooth(se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, linetype = 2) +
  labs(x = "Age",
       y = "Partial residual for age")

ggplot(slid_diag, aes(yearsEducation, yearsEducation_resid)) +
  geom_point(alpha = .1) +
  geom_smooth(se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, linetype = 2) +
  labs(x = "Education (years)",
       y = "Partial residual for education")
```

## Correcting for nonlinearity

$$\log(\text{Wage}) = \beta_0 + \beta_1(\text{Male}) + \beta_2 \text{Age} + \beta_3 \text{Age}^2 + \beta_4 \text{Education}^2$$

```{r slid-part-transform}
slid_log_trans <- lm(wage_log ~ sex + I(yearsEducation^2) + age + I(age^2), data = slid)
tidy(slid_log_trans)
```

## Correcting for nonlinearity

* Partial residuals

    $$\hat{\epsilon}_i^{\text{Age}} = `r coef(slid_log_trans)[[4]]` \times \text{Age}_i `r formatC(coef(slid_log_trans)[[5]])` \times \text{Age}^2_i + \hat{\epsilon}_i$$
    
    $$\hat{\epsilon}_i^{\text{Education}} = `r coef(slid_log_trans)[[3]]` \times \text{Education}^2_i + \hat{\epsilon}_i$$

* Partial fits

    $$\hat{Y}_i^{(\text{Age})} = `r coef(slid_log_trans)[[4]]` \times \text{Age}_i `r formatC(coef(slid_log_trans)[[5]])` \times \text{Age}^2_i$$
    
    $$\hat{Y}_i^{(\text{Education})} = `r coef(slid_log_trans)[[3]]` \times \text{Education}^2_i$$

## Correcting for nonlinearity {.scrollable}

```{r slid-part-trans-plot}
# get partial resids
slid_trans_resid <- residuals(slid_log_trans, type = "partial") %>%
  as_tibble
names(slid_trans_resid) <- c("sex", "education", "age", "age_sq")
names(slid_trans_resid) <- str_c(names(slid_trans_resid), "_resid")

slid_trans_diag <- augment(slid_log_trans, slid) %>%
  as_tibble %>%
  mutate(age_resid = coef(slid_log_trans)[[4]] * age +
           coef(slid_log_trans)[[5]] * age^2,
         educ_resid = coef(slid_log_trans)[[5]] * yearsEducation^2)

ggplot(slid_trans_diag, aes(age, age_resid + .resid)) +
  geom_point(alpha = .1) +
  geom_smooth(aes(y = age_resid), se = FALSE) +
  geom_smooth(se = FALSE, linetype = 2) +
  labs(x = "Age",
       y = "Partial residual for age")

ggplot(slid_trans_diag, aes(yearsEducation, educ_resid + .resid)) +
  geom_point(alpha = .1) +
  geom_smooth(aes(y = educ_resid), se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, linetype = 2) +
  labs(x = "Education (years)",
       y = "Partial residual for education")
```

## Collinearity

> A state of a model where explanatory variables are correlated with one another

## Perfect collinearity

```{r mtcars}
mtcars1 <- lm(mpg ~ disp + wt + cyl, data = mtcars)
summary(mtcars1)
```

## Perfect collinearity

```{r mtcars-recenter}
mtcars <- mtcars %>%
  mutate(disp_mean = disp - mean(disp))

mtcars2 <- lm(mpg ~ disp + wt + cyl + disp_mean, data = mtcars)
summary(mtcars2)
```

## Perfect collinearity

```{r mtcars-cor}
ggplot(mtcars, aes(disp, disp_mean)) +
  geom_point()
```

## Less-than-perfect collinearity

```{r credit}
credit <- read_csv("../data/Credit.csv") %>%
  select(-X1)
names(credit) <- tolower(names(credit))

ggplot(credit, aes(limit, age)) +
  geom_point()
```

## Less-than-perfect collinearity

```{r credit-lm}
age_limit <- lm(balance ~ age + limit, data = credit)
tidy(age_limit)
```

## Less-than-perfect collinearity {.scrollable}

```{r add-rating}
ggplot(credit, aes(rating, balance)) +
  geom_point() +
  geom_smooth()

limit_rate <- lm(balance ~ limit + rating, data = credit)
tidy(limit_rate)

coefplot::multiplot(age_limit, limit_rate)
```

## Less-than-perfect collinearity {.scrollable}

```{r limit-rate}
ggplot(credit, aes(limit, rating)) +
  geom_point() +
  geom_smooth()

coefplot::multiplot(age_limit, limit_rate, predictors = "limit")
```

## Correlation matrix

```{r credit-cor-mat}
library(GGally)

ggcorr(select_if(credit, is.numeric))
```

## Scatterplot matrix

```{r credit-scatter-mat}
ggpairs(select_if(credit, is.numeric))
```

## Variance inflation factor (VIF)

* Ratio of the variance of $\hat{\beta}_{1j}$ when fitting the full model divided by the variance of $\hat{\beta}_{1j}$ if fit on its own model
* Rule of thumb - greater than 10

## Variance inflation factor (VIF)

```{r vif}
vif(age_limit)
vif(limit_rate)
```

## Fixing multicollinearity

* Drop one or more of the collinear variables from the model - NO!
* Add more data
* Transform the variables
* Shrinkage methods
