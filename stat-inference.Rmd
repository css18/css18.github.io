---
title: "Models, statistical inference, and learning"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, cache = FALSE, message = FALSE}
library(tidyverse)
library(broom)
library(patchwork)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}}$$

# Statistical inference

**Statistical inference** is the process of using data to infer the probability distribution/random variable that generated the data. Given a sample $X_1, \ldots, X_n \sim F$, how do we infer $F$? Sometimes we want to infer all the features/parameters of $F$, and sometimes we only need a subset of those features/parameters.

# Parametric vs. nonparametric models

A **statistical model** $\xi$ is a set of distributions (or densities or regression functions). A **parametric model** is a set $\xi$ that can be parameterized by a finite number of parameters. We have seen many examples of parametric models - all the major types of random variables we've explored are defined in terms of a fixed number of parameters. For instance, if we assume that the data is generated by a Normal distribution, then the model is

$$\xi \equiv f(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[ -\frac{1}{2\sigma^2} (x - \mu)^2 \right], \quad \mu \in \Re, \sigma > 0$$

This is an example of a two-parameter model. The density $f(x; \mu, \sigma)$ indicates that $x$ is a value of the random variable $X$, whereas $\mu$ and $\sigma$ are parameters that define the model.

In general, a **parametric model** takes the form

$$\xi \equiv f(x; \theta) : \theta \in \Theta$$

where $\theta$ is an unknown parameter (or vector of parameters) that can only take values in the parameter space $\Theta$. If $\theta$ is a vector but we are only interested in one component of $\theta$, then we call the remaining parameters **nuisance parameters**.

A **nonparametric model** is a set $\xi$ that cannot be parameterized by a finite number of parameters.

## Examples of parametric models

### One-dimensional parametric estimation

Let $X_1, \ldots, X_n$ be independent observations drawn from a Bernoulli random variable with probability $\pi$ of success. The problem is to estiamte the parameter $\pi$.

### Two-dimensional parametric estimation

Suppose that $X_1, \ldots, X_n \sim F$ and we assume that the PDF $f \in \xi$ where $\xi \equiv f(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[ -\frac{1}{2\sigma^2} (x - \mu)^2 \right], \quad \mu \in \Re, \sigma > 0$. In this case, there are two parameters, $\mu$ and $\sigma$. The goal is to estimate the parameters from the data. If we are only interested in estimating $\mu$ (which is generally the case for inferential methods such as linear regression), then $\mu$ is the parameter of interest and $\sigma$ is a nuisance parameter.

## Nonparametric density estimation

Let $X_1, \ldots, X_n$ be independent observations from a cumulative distribution function (CDF) $F$ and let $f = F'$ be the probability density function (PDF)> Suppose we want to estimate the PDF $f$. It is not possible to estimate $f$ assuming only that $F \in \xi_{\text{ALL}}$ where $\xi_{\text{ALL}} = \{\text{all CDF's} \}$ -- that is, the sample space includes an infinite range of possible functions for the CDF. Instead, we need to assume some smoothness on $f$. We might assume that $f \in \xi = \xi_{\text{DENS}} \cap \xi_{\text{SOB}}$ where $\xi_{\text{DENS}}$ is the set of all PDFs and

$$\xi_{\text{SOB}} \equiv f: \int (f''(x))^2 dx < \infty$$

$\xi_{\text{SOB}}$ is a class called a **Sobolev space**, which is a set of functions that are the antiderivative of the square of the second derivative of $f$. In layman's terms, these are a group of functions that are not "too wiggly". The general form to calculate the density estimate is:

$$g(x) = \frac{1}{nh} \sum_{i = 1}^n f(x)$$

where $n$ is the number of observations, $h$ is the **bandwidth**, and $f(z)$ is a function drawn from the Sobolev space. There are many different functions that fall within the Sobolev space. For example, the **Gaussian kernel**

$$f(x) = \frac{1}{\sqrt{2 \pi}}\exp\left[-\frac{1}{2} x^2 \right]$$

is one such function. Note its strong relation to the Guassian (Normal) distribution. 
```{r infant-data}
infant <- read_csv("data/infant.csv") %>%
  # remove non-countries
  filter(is.na(`Value Footnotes`) | `Value Footnotes` != 1) %>%
  select(`Country or Area`, Year, Value) %>%
  rename(country = `Country or Area`,
         year = Year,
         mortal = Value)
```

```{r infant-hist}
ggplot(infant, aes(mortal)) +
  geom_histogram(bins = 30, boundary = 0) +
  labs(title = "Histogram of infant mortality rate for 195 nations",
       subtitle = "30 bins, origin = 0",
       x = "Infant mortality rate (per 1,000)",
       y = "Frequency")
```

```{r gaussian}
x <- rnorm(1000)

{
  qplot(x, geom = "blank") +
    stat_function(fun = dnorm) +
    labs(title = "Gaussian (normal) kernel",
         x = NULL,
         y = NULL)
} +
{
  ggplot(infant, aes(mortal)) +
    geom_density(kernel = "gaussian") +
    labs(title = "Infant mortality rate for 195 nations",
         x = "Infant mortality rate (per 1,000)",
         y = "Density")
}
```

Now we have a much smoother density function. Another such function is the **rectangular (uniform) kernel**:

$$f(x) = \frac{1}{2} \mathbf{1}_{\{ |x| \leq 1 \} }$$

where $\mathbf{1}_{\{ |x| \leq 1 \} }$ is an indicator function that takes on the value of 1 if the condition is true ($|x| \leq 1$) or 0 if the condition is false. This is also known as the **naive density estimator**. Again, notice its similarities to the uniform distribution.

```{r uniform}
x <- runif(1000, -1.5, 1.5)
x_lines <- tribble(
  ~x, ~y, ~xend, ~yend,
  -1, 0, -1, .5,
  1, 0, 1, .5
)

{
  qplot(x, geom = "blank") +
    stat_function(fun = dunif, args = list(min = -1), geom = "step") +
    # geom_segment(data = x_lines, aes(x = x, y = y, xend = xend, yend = yend)) +
    labs(title = "Rectangular kernel",
         x = NULL,
         y = NULL)
} +
{
  ggplot(infant, aes(mortal)) +
    geom_density(kernel = "rectangular") +
    labs(title = "Infant mortality rate for 195 nations",
         x = "Infant mortality rate (per 1,000)",
         y = "Density")
}
```

```{r kernels}
# define custom kernel functions
triangular <- function(x) {
  (1 - abs(x)) * ifelse(abs(x) <= 1, 1, 0)
}

biweight <- function(x) {
  (15 / 16) * (1 - x^2)^2 * ifelse(abs(x) <= 1, 1, 0)
}

epanechnikov <- function(x) {
  (15 / 16) * (1 - x^2)^2 * ifelse(abs(x) <= 1, 1, 0)
}

qplot(x, geom = "blank") +
  stat_function(aes(color = "Gaussian"), fun = dnorm) +
  stat_function(aes(color = "Epanechnikov"), fun = epanechnikov) +
  stat_function(aes(color = "Rectangular"), fun = dunif,
                args = list(min = -1), geom = "step") +
  stat_function(aes(color = "Triangular"), fun = triangular) +
  stat_function(aes(color = "Biweight"), fun = biweight) +
  scale_color_brewer(type = "qual") +
  labs(x = NULL,
       y = NULL,
       color = NULL) +
  theme(legend.position = c(0.04, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(fill = "white"))

ggplot(infant, aes(mortal)) +
  geom_density(aes(color = "Gaussian"), kernel = "gaussian") +
  geom_density(aes(color = "Epanechnikov"), kernel = "epanechnikov") +
  geom_density(aes(color = "Rectangular"), kernel = "rectangular") +
  geom_density(aes(color = "Triangular"), kernel = "triangular") +
  geom_density(aes(color = "Biweight"), kernel = "biweight") +
  scale_color_brewer(type = "qual") +
  labs(title = "Density estimators of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density",
       color = "Kernel") +
  theme(legend.position = c(0.96, 1),
        legend.justification = c(1, 1),
        legend.background = element_rect(fill = "white"))
```

## Parametric vs. nonparametric regression

Suppose we observe pairs of data $(X_1, Y_1), \ldots, (X_n, Y_n)$. $X$ is called the **predictor/regressor/feature/independent variable**. $Y$ is the **outcome/response variable/dependent variable**. We call $r(x) = \E(Y | X = x)$ the **regression function**. If we assume that $r \in \xi$ where $\xi$ is finite dimensional, then we have a **parametric regression model**. OLS is a classic example of parametric regression, where we assume a monotonic, linear relationship between the features and outcome of interest and attempt to estimate the parameter values (coefficients) from the data. Nonparametric regression makes no assumptions about the functional form of $r(x)$, so $\xi$ is infinite. While more flexible, this makes estimating the regression model far more difficult.

Suppose we have detailed information on individuals' wages and education. We don't have data for the entire population, but we do have observations for one million employed Americans:

```{r np-data}
n <- 1000000
wage <- data_frame(educ = rpois(n, lambda = 12),
                   age = rpois(n, lambda = 40),
                   prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

ggplot(wage, aes(wage)) +
  geom_histogram(binwidth = 5) +
  labs(title = "Histogram of simulated income data",
       subtitle = "Binwidth = 5",
       x = "Income, in thousands of dollars",
       y = "Frequency count")
```

If we want to estimate the average income for any individual in the dataset, let $X_1, \ldots, X_n \sim F$ where $X$ is income and it follows some known distribution $F$. The average value of that random variable $\mu = \E (X) = \int x F(x) dx$ assuming only that $\mu$ exists. The mean $\mu$ (also known as the **first moment**) can be thought of as a function of $F$: we can write $\mu = T(F) = \int x F(x) dx$. In general, any function of $F$ is called a **statistical function**. Because the method for calculating the statistical function does not depend on the form of $F$ itself, it is a nonparametric estimation technique.

If we want to estimate the income for an individual given their education level $(0, 1, 2, \dots, 25)$, we could estimate the conditional distribution of income for each of these values:

$$\mu = E(\text{Income}|\text{Education}) = f(\text{Income}|\text{Education})$$

For each level of education, the conditional (or expected) income would be the mean or median of all individuals in the sample with the same level of education.

```{r np-wage-cond}
wage %>%
  group_by(educ) %>%
  summarize(mean = mean(wage),
            sd = sd(wage)) %>%
  ggplot(aes(educ, mean, ymin = mean - sd, ymax = mean + sd)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "Conditional income, by education level",
       subtitle = "Plus/minus SD",
       x = "Education level",
       y = "Income, in thousands of dollars")

wage %>%
  filter(educ == 12) %>%
  ggplot(aes(wage)) +
  geom_density() +
  geom_vline(xintercept = mean(wage$wage[wage$educ == 12]), linetype = 2) +
  labs(title = "Conditional distribution of income for education = 12",
       subtitle = str_c("Mean income = ", formatC(mean(wage$wage[wage$educ == 12]), digits = 3)),
       x = "Income, in thousands of dollars",
       y = "Frequency count")
```

Imagine instead that we have $X$ and $Y$, two continuous variables from a sample of a population, and we want to understand the relationship between the variables. Specifically, we want to use our knowledge of $X$ to predict $Y$. Therefore what we want to know is the mean value of $Y$ as a function of $X$ in the population of individuals from whom the sample was drawn:

$$\mu = E(Y|x) = f(x)$$

Unfortunately because $X$ is continuous, it is unlikely that we would draw precisely the same values of $X$ for more than a single observation. Therefore we cannot directly calculate the conditional distribution of $Y$ given $X$, and therefore cannot calculate conditional means. Instead, we can divide $X$ into many narrow intervals (or **bins**), just like we would for a histogram. Within each bin we can estimate the conditional distribution of $Y$ and estimate the conditional mean of $Y$ with great precision.

If we have fewer observations, then we have to settle for fewer bins and less precision in our estimates. Here we use data on the average income of 102 different occupations in Canada and their relationship to occupational prestige (measured continuously):

```{r prestige}
# get data
prestige <- read_csv("data/prestige.csv")
```

```{r prestige-5bins, dependson="prestige"}
# bin into 5 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 6)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
  as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[2:(length(.)-1)] %>%
  as_tibble

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 5",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

The $X$-axis is carved into 5 bins with roughly 20 observations in each bin. The line is a **naive nonparametric regression line** that is calculated by connecting the points defined by the conditional variable means $\bar{Y}$ and the explanatory variable means $\bar{X}$ in the five intervals.

Just like ordinary least squares regression (OLS), this regression line also suffers from **bias** and **variance**. If the actual relationship between prestige and income is non-linear **within a bin**, then our estimate of the conditional mean $\bar{Y}$ will be biased towards a linear relationship. We can minimize bias by making the bins as numerous and narrow as possible:

```{r prestige-50bins, dependson="prestige"}
# bin into 50 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 51)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
  as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[2:(length(.)-1)] %>%
  as_tibble

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2, alpha = .25) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 50",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

But now we have introduced overfitting into the nonparametric regression estimates. In addition, we substantially increased our variance of the estimated conditional sample means $\bar{Y}$. If we were to draw a new sample, the estimated conditional sample means $\bar{Y}$ could be widely different from the original model and our resulting estimates of the conditional sample means will be highly variable.

Naive nonparametric regression is a consistent estimator of the population regression curve as the sample size increases. As $n \rightarrow \infty$, we can shrink the size of the individual intervals and still have sizeable numbers of observations in each interval. In the limit, we have an infinite number of intervals and infinite number of observations in each interval, so the naive nonparametric regression line and the population regression line are identical.

As a practical consideration, if our sample size $n$ is truly large, then naive nonparametric regression could be a good estimation procedure. However as we introduce multiple explanatory variables into the model, the problem starts to blow up. Assume we have three discrete explanatory variables each with 10 possible values:

$$
\begin{align}
X_1 &\in \{1, 2, \dots ,10 \} \\
X_2 &\in \{1, 2, \dots ,10 \} \\
X_3 &\in \{1, 2, \dots ,10 \}
\end{align}
$$

There are then $10^3 = 1000$ possible combinations of the explanatory variables and $1000$ conditional expectations of $Y$ given $X$:

$$\mu = E(Y|x_1, x_2, x_3) = f(x_1, x_2, x_3)$$

In order to accurate estimate conditional expectations for each category, we would need substantial numbers of observations **for every combination of $X$**. This would require a sample size far greater than most social scientists have the resources to collect.

Let's return to our simulated wage data. Our dataset contains information on education, age, and job prestige:

```{r wage-sim-describe}
ggplot(wage, aes(educ)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Education",
       y = "Frequency count")

ggplot(wage, aes(age)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Age",
       y = "Frequency count")

ggplot(wage, aes(prestige)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Job prestige",
       y = "Frequency count")
```

Can we estimate naive nonparametric regression on this dataset with $N = 1,000,000$?

```{r wage-sim-np}
wage_np <- wage %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                          wage_sd = NA,
                                          n = 0))

# number of unique combos 
wage_unique <- nrow(wage_np)

# n for each unique combo
ggplot(wage_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

Even on a dataset with $1,000,000$ observations, for the vast majority of the potential combinations of variables we have zero observations from which to generate expected values. What if we instead drew $10,000,000$ observations from the same data generating process?

```{r wage-sim-np-ten}
n <- 10000000
wage10 <- data_frame(educ = rpois(n, lambda = 12),
                   age = rpois(n, lambda = 40),
                   prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

wage10_np <- wage10 %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                          wage_sd = NA,
                                          n = 0))

# number of unique combos 
wage10_unique <- nrow(wage10_np)

# n for each unique combo
ggplot(wage10_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

Unless your dataset is extremely large or you have a small handful of variables with a low number of unique values, naive nonparametric estimation will not be effective.

# Point estimates


# Confidence sets


# Hypothesis testing



# Acknowledgements {.toc-ignore}

* Material drawn from [**All of Statistics**](https://link-springer-com.proxy.uchicago.edu/book/10.1007%2F978-0-387-21736-9) by Larry Wasserman

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```
