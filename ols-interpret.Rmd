---
title: "Ordinary least squares"
subtitle: "Interpretation and hypothesis testing"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, cache = FALSE, message = FALSE}
library(tidyverse)
library(broom)
library(sf)
library(albersusa)

options(digits = 3)
set.seed(1234)

theme_set(theme_minimal())
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

# Bivariate regression

Consider cigarette consumption in the United States. Here we have a dataset of state-level statistics in the United States from 2008.

```{r import-states}
states <- haven::read_dta("data/states.dta")
```

One column `cigarettes` reports the number of packs of cigarettes purchased bimonthly per adult in each of the 50 US states.

```{r cig}
ggplot(states, aes(cigarettes)) +
  geom_histogram() +
  labs(title = "Cigarette consumption in the United States",
       subtitle = "By state",
       x = "Bimonthly packs of cigarettes per adult",
       y = "Number of states")
```

Let's say we want to explain cigarette consumption. What factors or independent variables do we think may have an influence on cigarette consumption? One possibility is a **cigarette tax**.

* $H_1$ -- In a comparison of states, those having higher taxes on cigarettes have fewer purchases of cigarettes than states with lower taxes on cigarettes.

```{r cig-tax}
ggplot(states, aes(cig_tax)) +
  geom_histogram() +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Cigarette taxes in the United States",
       x = "Cigarette tax (per pack)",
       y = "Number of states")
```

```{r cig-tax-lm}
ggplot(states, aes(cig_tax, cigarettes)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Cigarette consumption in the United States",
       x = "Cigarette tax (per pack)",
       y = "Bimonthly packs of cigarettes per adult")

cig_tax <- lm(cigarettes ~ cig_tax, data = states)
summary(cig_tax)
```

* Interpret $\boldsymbol{\hat{\beta}}$
* Generate predicted values
* Null hypothesis test
* $R^2$

# Multivariable regression

Now add a second hypothesis to the model:

* $H_2$ -- In a comparison of states, Southern states have more purchases of cigarettes than non-Southern states.

```{r south}
# get simple features and join with states
us_sf <- usa_sf("laea") %>%
  left_join(states, by = c("name" = "state")) %>%
  filter(name != "District of Columbia")

us_sf %>%
  mutate(cigarettes = cut_number(cigarettes, n = 6)) %>%
  ggplot() +
  geom_sf(aes(fill = cigarettes)) +
  scale_fill_brewer() +
  labs(title = "Cigarette consumption in the United States",
       subtitle = "Bimonthly packs of cigarettes per adult",
       fill = NULL)
```

```{r south-lm}
south <- lm(cigarettes ~ cig_tax + south, data = states)
summary(south)
```

* Interpret $\boldsymbol{\hat{\beta}}$
* Generate predicted values
* Null hypothesis test
* $R^2$

# Estimating linear regression models in R

To estimate linear models in R, we use the `lm()` function:

```{r south-lm}
```

The `lm()` function takes two parameters. The first is a **formula** specifying the equation to be estimated (`lm()` translates `y ~ x` into $y = \beta_0 + \beta_1 * x$). The second is of course the data frame containing the variables.

Note that we have now begun to leave the `tidyverse` universe. `lm()` is part of the base R program, and the result of `lm()` is decidedly **not tidy**.

```{r lm-str}
str(south)
```

The result is stored in a complex list that contains a lot of important information, some of which you may recognize but most of it you do not. In order to extract model statistics and use them in a **tidy** manner, we can use a set of functions from the `broom` package. For these functions, the input is always the model object itself, not the original data frame.

## `tidy()`

`tidy()` constructs a data frame that summarizes the model's statistical findings. This includes **coefficients** and **p-values** for each parameter in a regression model. Note that depending on the statistical learning method employed, the statistics stored in the columns may vary.

```{r tidy}
tidy(south)

tidy(south) %>%
  str()
```

Notice that the structure of the resulting object is a tidy data frame. Every row contains a single parameter, every column contains a single statistic, and every cell contains exactly one value.

## `augment()`

`augment()` adds columns to the original data that was modeled. This could include predictions, residuals, and other observation-level statistics.

```{r augment}
augment(south) %>%
  as_tibble()
```

`augment()` will return statistics to the original data used to estimate the model, however if you supply a data frame under the `newdata` argument, it will return a more limited set of statistics.

## `glance()`

`glance()` constructs a concise one-row summary of the model. This typically contains values such as $R^2$, adjusted $R^2$, and residual standard error that are computed once for the entire model.

```{r glance}
glance(south)
```

While `broom` may not work with every model in R, it is compatible with a wide range of common statistical models. A full list of models with which `broom` is compatible can be found on the [GitHub page for the package](https://github.com/dgrtwo/broom).

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```
